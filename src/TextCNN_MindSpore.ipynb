{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:root:Using MoXing-v2.0.0.rc2.4b57a67b-4b57a67b\n","INFO:root:Using OBS-Python-SDK-3.20.9.1\n"]}],"source":["import moxing as mox\n","mox.file.copy_parallel(src_url=\"s3://data96/text_classification_mindspore/data/\", dst_url='./data/') \n","mox.file.copy_parallel(src_url=\"s3://data96/text_classification_mindspore/word2vec/\", dst_url='./word2vec/') "]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n","Collecting gensim\n","  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/da/46/e4fb31929d8871adc90ed85266e8418666020c09bbeb60b8e5544edb1a7e/gensim-4.1.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (24.0 MB)\n","\u001b[K     |████████████████████████████████| 24.0 MB 50.9 MB/s eta 0:00:01\n","\u001b[?25hCollecting smart-open>=1.8.1\n","  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/cd/11/05f68ea934c24ade38e95ac30a38407767787c4e3db1776eae4886ad8c95/smart_open-5.2.1-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 15.0 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ma-user/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages (from gensim) (1.17.5)\n","Requirement already satisfied: scipy>=0.18.1 in /home/ma-user/miniconda3/envs/MindSpore-python3.7-aarch64/lib/python3.7/site-packages (from gensim) (1.5.4)\n","Installing collected packages: smart-open, gensim\n","Successfully installed gensim-4.1.2 smart-open-5.2.1\n"]}],"source":["! pip install gensim"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:gensim.models.keyedvectors:loading projection weights from ./word2vec/wiki-news-300d-1M.vec\n","INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (999994, 300) matrix of type float32 from ./word2vec/wiki-news-300d-1M.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-04-06T12:42:57.331481', 'gensim': '4.1.2', 'python': '3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:15:32) \\n[GCC 7.5.0]', 'platform': 'Linux-4.19.36-vhulk1907.1.0.h619.eulerosv2r8.aarch64-aarch64-with-centos-2.0-SP8', 'event': 'load_word2vec_format'}\n"]},{"name":"stdout","output_type":"stream","text":["Negative reivews:\n","[0]:simplistic , silly and tedious . \n","\n","[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n","\n","[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n","\n","[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n","\n","[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n","\n","Positive reivews:\n","[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n","\n","[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n","\n","[2]:effective but too-tepid biopic\n","\n","[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n","\n","[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n","\n"]}],"source":["import math\n","import numpy as np\n","import pandas as pd\n","import os\n","import io\n","import math\n","import random\n","import codecs\n","from pathlib import Path\n","import mindspore\n","import mindspore.dataset as ds\n","import mindspore.nn as nn\n","from mindspore import Tensor\n","from mindspore import context\n","from mindspore.train.model import Model\n","from mindspore.nn.metrics import Accuracy\n","from mindspore.train.serialization import load_checkpoint, load_param_into_net\n","from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n","from mindspore.ops import operations as ops\n","from easydict import EasyDict as edict\n","from multiprocessing import cpu_count\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","cfg = edict({\n"," 'name': 'movie review',\n"," 'pre_trained': False,\n"," 'num_classes': 2,\n"," 'batch_size': 64,\n"," 'epoch_size': 4,\n"," 'weight_decay': 3e-5,\n"," 'data_path': './data/',\n"," 'device_target': 'Ascend',\n"," 'device_id': 0,\n"," 'keep_checkpoint_max': 1,\n"," 'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n"," 'word_len': 51,\n"," 'vec_length': 200\n","})\n","\n","word2vec_model = KeyedVectors.load_word2vec_format(\"./word2vec/wiki-news-300d-1M.vec\")\n","context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)\n","with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n","    print(\"Negative reivews:\")\n","    for i in range(5):\n","        print(\"[{0}]:{1}\".format(i,f.readline()))\n","\n","with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n","    print(\"Positive reivews:\")\n","    for i in range(5):\n","        print(\"[{0}]:{1}\".format(i,f.readline()))\n","  "]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["class Generator():\n","    def __init__(self, input_list):\n","        self.input_list=input_list\n","    def __getitem__(self,item):\n","        return (np.array(self.input_list[item][0],dtype=np.float32),\n","                np.array(self.input_list[item][1],dtype=np.int32))\n","    def __len__(self):\n","        return len(self.input_list)\n","\n","\n","class MovieReview:\n","    '''\n","    影评数据集\n","    '''\n","    def __init__(self, root_dir, maxlen, split):\n","        '''\n","        input:\n","            root_dir: 影评数据目录\n","            maxlen: 设置句子最大长度\n","            split: 设置数据集中训练/评估的比例\n","        '''\n","        self.path = root_dir\n","        self.feelMap = {\n","            'neg':0,\n","            'pos':1\n","        }\n","        self.files = []\n","\n","        self.doConvert = False\n","        \n","        mypath = Path(self.path)\n","        if not mypath.exists() or not mypath.is_dir():\n","            print(\"please check the root_dir!\")\n","            raise ValueError\n","\n","        # 在数据目录中找到文件\n","        for root,_,filename in os.walk(self.path):\n","            for each in filename:\n","                self.files.append(os.path.join(root,each))\n","            break\n","\n","        # 确认是否为两个文件.neg与.pos\n","        if len(self.files) != 2:\n","            print(\"There are {} files in the root_dir\".format(len(self.files)))\n","            raise ValueError\n","\n","        # 读取数据\n","        self.word_num = 0\n","        self.maxlen = 0\n","        self.minlen = float(\"inf\")\n","        self.maxlen = float(\"-inf\")\n","        self.Pos = []\n","        self.Neg = []\n","        for filename in self.files:\n","            f = codecs.open(filename, 'r')\n","            ff = f.read()\n","            file_object = codecs.open(filename, 'w', 'utf-8')\n","            file_object.write(ff)\n","            self.read_data(filename)\n","        self.PosNeg = self.Pos + self.Neg\n","\n","        self.text2vec(maxlen=maxlen)\n","        self.split_dataset(split=split)\n","\n","    def read_data(self, filePath):\n","\n","        with open(filePath,'r') as f:\n","            \n","            for sentence in f.readlines():\n","                sentence = sentence.replace('\\n','')\\\n","                                    .replace('\"','')\\\n","                                    .replace('\\'','')\\\n","                                    .replace('.','')\\\n","                                    .replace(',','')\\\n","                                    .replace('[','')\\\n","                                    .replace(']','')\\\n","                                    .replace('(','')\\\n","                                    .replace(')','')\\\n","                                    .replace(':','')\\\n","                                    .replace('--','')\\\n","                                    .replace('-',' ')\\\n","                                    .replace('\\\\','')\\\n","                                    .replace('0','')\\\n","                                    .replace('1','')\\\n","                                    .replace('2','')\\\n","                                    .replace('3','')\\\n","                                    .replace('4','')\\\n","                                    .replace('5','')\\\n","                                    .replace('6','')\\\n","                                    .replace('7','')\\\n","                                    .replace('8','')\\\n","                                    .replace('9','')\\\n","                                    .replace('`','')\\\n","                                    .replace('=','')\\\n","                                    .replace('$','')\\\n","                                    .replace('/','')\\\n","                                    .replace('*','')\\\n","                                    .replace(';','')\\\n","                                    .replace('<b>','')\\\n","                                    .replace('%','')\n","                sentence = sentence.split(' ')\n","                sentence = list(filter(lambda x: x, sentence))\n","                if sentence:\n","                    self.word_num += len(sentence)\n","                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n","                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n","                    if 'pos' in filePath:\n","                        self.Pos.append([sentence,self.feelMap['pos']])\n","                    else:\n","                        self.Neg.append([sentence,self.feelMap['neg']])\n","    \n","    def load_vectors(fname):\n","        fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","        n, d = map(int, fin.readline().split())\n","        data = {}\n","        for line in fin:\n","            tokens = line.rstrip().split(' ')\n","            data[tokens[0]] = map(float, tokens[1:])\n","        return data\n","\n","    def text2vec(self, maxlen):\n","        '''\n","        将句子转化为向量\n","\n","        '''\n","        # Vocab = {word : index}\n","        self.Vocab = dict()\n","        # self.Vocab['None']\n","       \n","        for SentenceLabel in self.Pos+self.Neg:\n","            vector = np.zeros((maxlen, 300), dtype=np.float32)\n","            for index, word in enumerate(SentenceLabel[0]):\n","                if index >= maxlen:\n","                    break\n","                if word not in word2vec_model:\n","                    continue\n","                vector[index] = word2vec_model[word]\n","                \n","            SentenceLabel[0] = vector\n","        self.doConvert = True\n","\n","    def split_dataset(self, split):\n","        '''\n","        分割为训练集与测试集\n","\n","        '''\n","        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n","        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n","        trunk_num = int(1/(1-split))\n","        pos_temp=list()\n","        neg_temp=list()\n","        for index in range(trunk_num):\n","            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n","            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n","        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n","        self.train = [i for item in pos_temp+neg_temp for i in item]\n","\n","        random.shuffle(self.train)\n","        # random.shuffle(self.test)\n","\n","    def get_dict_len(self):\n","        '''\n","        获得数据集中文字组成的词典长度\n","        '''\n","        if self.doConvert:\n","            return 1\n","        else:\n","            print(\"Haven't finished Text2Vec\")\n","            return -1\n","\n","    def create_train_dataset(self, epoch_size, batch_size):\n","        dataset = ds.GeneratorDataset(\n","                                        source=Generator(input_list=self.train), \n","                                        column_names=[\"data\",\"label\"], \n","                                        shuffle=False\n","                                        )\n","#         dataset.set_dataset_size(len(self.train))\n","        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n","        dataset=dataset.repeat(epoch_size)\n","        return dataset\n","\n","    def create_test_dataset(self, batch_size):\n","        dataset = ds.GeneratorDataset(\n","                                        source=Generator(input_list=self.test), \n","                                        column_names=[\"data\",\"label\"], \n","                                        shuffle=False\n","                                        )\n","#         dataset.set_dataset_size(len(self.test))\n","        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n","        return dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["data  word2vec\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab_size:1\n","{'data': Tensor(shape=[64, 51, 300], dtype=Float32, value=\n","[[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 4.39000018e-02,  2.22000003e-01,  4.43999991e-02 ...  1.47699997e-01, -9.57000032e-02,  1.06399998e-01],\n","  [ 8.96999985e-02,  1.60000008e-02, -5.71000017e-02 ...  1.55900002e-01, -2.53999997e-02, -2.59000007e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[ 4.69999993e-03,  2.22999994e-02, -8.70000012e-03 ...  1.47900000e-01,  1.32400006e-01, -3.18000019e-02],\n","  [-1.17399998e-01, -1.54400006e-01,  3.75000015e-02 ...  2.09999993e-01, -1.11199997e-01,  3.19999992e-03],\n","  [-2.04899997e-01,  2.20500007e-01,  1.82000007e-02 ... -3.44999991e-02, -6.37999997e-02, -8.39999989e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[ 8.96999985e-02,  1.60000008e-02, -5.71000017e-02 ...  1.55900002e-01, -2.53999997e-02, -2.59000007e-02],\n","  [-2.25000009e-02, -2.49000005e-02,  1.63000003e-02 ...  2.38600001e-01, -1.07100002e-01, -1.28999993e-01],\n","  [ 7.60999992e-02, -9.79000032e-02,  2.60000005e-02 ...  2.27799997e-01, -4.94000018e-02, -1.32799998e-01],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," ...\n"," [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [-3.64000015e-02,  1.01400003e-01, -7.89000019e-02 ...  4.71000001e-02, -6.97999969e-02, -1.78999994e-02],\n","  [ 1.55999996e-02,  7.51999989e-02, -7.80000016e-02 ...  8.82000029e-02, -8.82000029e-02, -9.60000046e-03],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[ 4.69999993e-03,  2.22999994e-02, -8.70000012e-03 ...  1.47900000e-01,  1.32400006e-01, -3.18000019e-02],\n","  [ 1.21999998e-02, -1.45600006e-01, -8.10000021e-03 ...  1.77300006e-01, -1.24700002e-01, -4.03000005e-02],\n","  [-3.64000015e-02,  1.01400003e-01, -7.89000019e-02 ...  4.71000001e-02, -6.97999969e-02, -1.78999994e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[ 2.44100004e-01, -2.83000004e-02,  6.08000010e-02 ...  8.12000036e-02,  3.44000012e-02, -1.87900007e-01],\n","  [ 1.55999996e-02,  7.51999989e-02, -7.80000016e-02 ...  8.82000029e-02, -8.82000029e-02, -9.60000046e-03],\n","  [-1.28900006e-01,  1.15000000e-02,  3.29999998e-03 ...  1.81099996e-01,  3.97999994e-02, -5.29999994e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]]), 'label': Tensor(shape=[64], dtype=Int32, value= [1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, \n"," 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, \n"," 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0])}\n","[[ 0.0047  0.0223 -0.0087 ...  0.1479  0.1324 -0.0318]\n"," [-0.1174 -0.1544  0.0375 ...  0.21   -0.1112  0.0032]\n"," [-0.2049  0.2205  0.0182 ... -0.0345 -0.0638 -0.084 ]\n"," ...\n"," [ 0.      0.      0.     ...  0.      0.      0.    ]\n"," [ 0.      0.      0.     ...  0.      0.      0.    ]\n"," [ 0.      0.      0.     ...  0.      0.      0.    ]]\n","{'data': Tensor(shape=[64, 51, 300], dtype=Float32, value=\n","[[[ 4.69999993e-03,  2.22999994e-02, -8.70000012e-03 ...  1.47900000e-01,  1.32400006e-01, -3.18000019e-02],\n","  [-1.17200002e-01, -1.41800001e-01,  2.16000006e-02 ...  1.86100006e-01,  4.23000008e-02, -6.88999966e-02],\n","  [ 3.53000015e-02, -3.84000018e-02,  8.83999988e-02 ...  1.40400007e-01,  2.01999992e-02,  2.55999994e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[ 3.73000018e-02, -1.51199996e-01, -1.32900000e-01 ...  9.78000015e-02, -1.53999999e-02, -8.85000005e-02],\n","  [-1.92100003e-01, -2.21300006e-01,  1.25499994e-01 ...  1.53200001e-01,  7.80000016e-02,  2.19899997e-01],\n","  [-1.00800000e-01, -1.00000005e-03,  1.04400001e-01 ...  1.45099998e-01,  5.05000018e-02, -1.11699998e-01],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[-3.11600000e-01,  8.56000036e-02, -6.89999992e-03 ...  8.77000019e-02,  1.01899996e-01,  9.70000029e-03],\n","  [-1.06299996e-01, -5.71000017e-02, -1.19000003e-01 ...  3.18500012e-01, -3.73000018e-02, -1.35299996e-01],\n","  [-2.41500005e-01, -2.36000009e-02, -6.53999969e-02 ...  2.86599994e-01, -1.82899997e-01,  2.98999995e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," ...\n"," [[-6.39999984e-03, -2.84000002e-02,  1.11999996e-02 ...  5.48000000e-02, -5.49999997e-02,  9.99999975e-05],\n","  [ 8.20999965e-02,  2.18000002e-02, -6.50000013e-03 ...  3.62000018e-02, -5.20000011e-02, -1.90000003e-03],\n","  [-3.13999988e-02,  1.48999998e-02, -2.05000006e-02 ...  9.79999974e-02,  8.92999992e-02,  1.48000000e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[-1.53999999e-02, -2.00000009e-03, -7.24999979e-02 ...  1.85800001e-01,  1.04999997e-01, -4.23000008e-02],\n","  [ 2.76999995e-02, -4.87000011e-02,  6.08999990e-02 ...  2.64299989e-01, -2.09999997e-02,  4.14999984e-02],\n","  [ 9.62999985e-02, -3.10999993e-02,  1.44999996e-02 ...  8.21999982e-02, -1.45999998e-01,  8.69999975e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n"," [[ 5.24999984e-02,  1.14600003e-01,  3.81000005e-02 ...  1.90400004e-01,  2.96000000e-02, -3.50000011e-03],\n","  [-4.49999981e-03, -4.78999987e-02, -2.29000002e-02 ...  1.72700003e-01, -3.15000005e-02, -2.22999994e-02],\n","  [ 8.96999985e-02,  1.60000008e-02, -5.71000017e-02 ...  1.55900002e-01, -2.53999997e-02, -2.59000007e-02],\n","  ...\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","  [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, \n"," 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, \n"," 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0])}\n","[[ 0.0373 -0.1512 -0.1329 ...  0.0978 -0.0154 -0.0885]\n"," [-0.1921 -0.2213  0.1255 ...  0.1532  0.078   0.2199]\n"," [-0.1008 -0.001   0.1044 ...  0.1451  0.0505 -0.1117]\n"," ...\n"," [ 0.      0.      0.     ...  0.      0.      0.    ]\n"," [ 0.      0.      0.     ...  0.      0.      0.    ]\n"," [ 0.      0.      0.     ...  0.      0.      0.    ]]\n"]}],"source":["instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n","dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n","batch_num = dataset.get_dataset_size()\n","vocab_size=instance.get_dict_len()\n","print(\"vocab_size:{0}\".format(vocab_size))\n","item =dataset.create_dict_iterator()\n","for i,data in enumerate(item):\n","    if i<2:\n","        print(data)\n","        print(data['data'][1])\n","    else:\n","        break"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["learning_rate = []\n","warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n","            for i in range(math.floor(cfg.epoch_size / 5))]\n","shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n","            for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n","normal_run = [1e-3 for _ in range(batch_num) for i in \n","                range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n","                    - math.floor(cfg.epoch_size * 2 / 5))]\n","learning_rate = learning_rate + warm_up + normal_run + shrink"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["def _weight_variable(shape, factor=0.01):\n","    init_value = np.random.randn(*shape).astype(np.float32) * factor\n","    return Tensor(init_value)\n","\n","def make_conv_layer(kernel_size):\n","    weight_shape = (96, 1, *kernel_size)\n","    weight = _weight_variable(weight_shape)\n","    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n","                pad_mode=\"pad\", weight_init=weight, has_bias=True)\n","\n","\n","class TextCNN(nn.Cell):\n","    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n","        super(TextCNN, self).__init__()\n","        self.vec_length = vec_length\n","        self.word_len = word_len\n","        self.num_classes = num_classes\n","        self.unsqueeze = ops.ExpandDims()\n","        self.ReLu = nn.ReLU()\n","        self.embed_fc = nn.Dense(300, self.vec_length)\n","        self.slice = ops.Slice()\n","        self.layer1 = self.make_layer(kernel_height=3)\n","        self.layer2 = self.make_layer(kernel_height=4)\n","        self.layer3 = self.make_layer(kernel_height=5)\n","        self.concat = ops.Concat(1)\n","        self.fc = nn.Dense(96*3, self.num_classes)\n","        self.drop = nn.Dropout(keep_prob=0.5)\n","        self.print = ops.Print()\n","        self.reducemean = ops.ReduceMax(keep_dims=False)\n","\n","    def make_layer(self, kernel_height):\n","        return nn.SequentialCell(\n","            [\n","                make_conv_layer((kernel_height,self.vec_length)),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n","            ]\n","        )\n","    \n","    def construct(self,x):\n","        x = self.unsqueeze(x, 1)\n","        x = self.ReLu(self.embed_fc(x))\n","        x1 = self.layer1(x)\n","        x2 = self.layer2(x)\n","        x3 = self.layer3(x)\n","        x1 = self.reducemean(x1, (2, 3))\n","        x2 = self.reducemean(x2, (2, 3))\n","        x3 = self.reducemean(x3, (2, 3))\n","        x = self.concat((x1, x2, x3))\n","        x = self.drop(x)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n","         num_classes=cfg.num_classes, vec_length=cfg.vec_length)"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["# 优化器、损失函数、保存检查点、时间监视器等设置\n","opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n","        learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n","loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n","model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})\n","config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), \n","keep_checkpoint_max=cfg.keep_checkpoint_max)\n","time_cb = TimeMonitor(data_size=batch_num)\n","ckpt_save_dir = \"./ckpt\"\n","ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n","loss_cb = LossMonitor()"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch: 1 step: 596, loss is 0.40294343\n","epoch time: 103989.798 ms, per step time: 174.480 ms\n","epoch: 2 step: 596, loss is 0.19032332\n","epoch time: 52176.938 ms, per step time: 87.545 ms\n","epoch: 3 step: 596, loss is 0.21809569\n","epoch time: 52109.314 ms, per step time: 87.432 ms\n","epoch: 4 step: 596, loss is 0.07222549\n","epoch time: 52169.522 ms, per step time: 87.533 ms\n","train success\n"]}],"source":["model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n","print(\"train success\")"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\n","accuracy:  {'acc': 0.8212890625}\n"]}],"source":["# 导入训练生成的 checkpoint\n","checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'\n","# 生成测试数据集\n","dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n","# 定义评估损失、网络\n","loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n","net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n","            num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n","if checkpoint_path is not None:\n","    param_dict = load_checkpoint(checkpoint_path)\n","    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n","else:\n","    param_dict = load_checkpoint(cfg.checkpoint_path)\n","    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n","load_param_into_net(net, param_dict)\n","net.set_train(False)\n","model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n","\n","acc = model.eval(dataset)\n","print(\"accuracy: \", acc)"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess(sentence):\n","    sentence = sentence.lower().strip()\n","    sentence = sentence.replace('\\n','')\\\n","                                            .replace('\"','')\\\n","                                            .replace('\\'','')\\\n","                                            .replace('.','')\\\n","                                            .replace(',','')\\\n","                                            .replace('[','')\\\n","                                            .replace(']','')\\\n","                                            .replace('(','')\\\n","                                            .replace(')','')\\\n","                                            .replace(':','')\\\n","                                            .replace('--','')\\\n","                                            .replace('-',' ')\\\n","                                            .replace('\\\\','')\\\n","                                            .replace('0','')\\\n","                                            .replace('1','')\\\n","                                            .replace('2','')\\\n","                                            .replace('3','')\\\n","                                            .replace('4','')\\\n","                                            .replace('5','')\\\n","                                            .replace('6','')\\\n","                                            .replace('7','')\\\n","                                            .replace('8','')\\\n","                                            .replace('9','')\\\n","                                            .replace('`','')\\\n","                                            .replace('=','')\\\n","                                            .replace('$','')\\\n","                                            .replace('/','')\\\n","                                            .replace('*','')\\\n","                                            .replace(';','')\\\n","                                            .replace('<b>','')\\\n","                                            .replace('%','')\\\n","                                            .replace(\" \",\" \")\n","    sentence = sentence.split(' ')\n","    maxlen = cfg.word_len\n","    vector = [0]*maxlen\n","    for index, word in enumerate(sentence):\n","        if index >= maxlen:\n","            break\n","        if word not in instance.Vocab.keys():\n","            print(word,\"单词未出现在字典中\")\n","        else:\n","            vector[index] = instance.Vocab[word]\n","    sentence = vector\n","    return sentence\n","\n","def inference(review_en):\n","    review_en = preprocess(review_en)\n","    input_en = Tensor(np.array([review_en]).astype(np.int32))\n","    output = net(input_en)\n","    if np.argmax(np.array(output[0])) == 1:\n","        print(\"Positive comments\")\n","    else:\n","        print(\"Negative comments\")"]}],"metadata":{"kernelspec":{"display_name":"MindSpore-python3.7-aarch64","language":"python","name":"mindspore-python3.7-aarch64"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}
